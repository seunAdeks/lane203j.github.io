---
layout: post
title: Restricted Boltzmann Machines Part 1, Markov Random Fields
---

In this series of posts, I will give an introduction to restricted Boltzmann machines, eventually discussing their implementation and applications in machine learning. Before getting to applications, we will build some of the underlying mathematical theory. 

## Graphical models and the local Markov property

Markov random fields (MRFs) are a specific kind of probablistic model called a *graphical model*. Roughly speaking, a graphical model is a graph whose vertices correspond to random variables (usually from the same distribution) and whose edges carry information about conditional dependence of those random variables. Graphical models are useful for studying joint probability distributions with many variables because they generally have fewer parameters than alternatives.

Before giving the precise definition of MRFs, let's recall some background. Random variables \\( x,y \\) are *conditionally independent given* \\( z \\) if 
\\[
p(x,y\vert z) = p(x\vert z) p(y\vert z).
\\]
By the definition of conditional probability, this is equivalent to either of the equations
\\[
p(x\vert y,z) = p(x\vert z)
\\] 
or 
\\[ 
p(y\vert x,z) = p(y\vert z).
]\\

One well-known example of a (directed) graphical model is a *Markov chain*. A sequence of random variables \\( x_t \\),  \\( t \\) a positive integer, is a Markov chain if every state only depends on the previous state, not earlier ones. This condition is called the *Markov property.* In terms of conditional independence, this is the statement that each \\(x_{t_0}\\) is conditionally independent of all earlier \\(x_t\\)'s, given \\( x_{t_0-1}\\).

A Markov chain can be represented by a graph with vertex set \\( V = \mathbb{N} \\) and directed edges from \\(v_i\\) to \\(v_{i+1}\\). In the terminology of graph theory, the Markov property for a Markov chain can be rephrased as the more general *local Markov property*.

> **local Markov property for directed graphical models:** Each random variable \\(x_{v}\\) is conditionally independent of all its non-descendant random variables (there is no directed path from \\(v\\) to the vertex), given the random variables \\(x_w\\) such that \\(w\\) is a parent of \\(v\\) (there is a directed edge from \\(w\\) to \\(v\\)).

This more general definition makes sense on any acyclic directed graph, not just the one used for Markov chains. A Bayesian network is simply a directed graphical model that satisfies the local Markov property above. We won't focus on these models for the rest of this post, but it's worth being aware of them.

A Markov random field is an *undirected* graphical model (i.e. the edges are not directed) that has a similar local Markov property. 

> **local Markov property for undirected graphical models:** Each random variable \\(x_{v}\\) is conditionally independent of all other random variables, given the random variables \\(x_w\\) such that \\(w\\) is adjacent to \\(v\\) (there is an edge from \\(w\\) to \\(v\\)).

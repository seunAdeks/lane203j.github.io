---
layout: post
title: Restricted Boltzmann Machines Part 2, the distributions of Boltzmann machines
---

This post is a continuation from [part 1](https://lane203j.github.io/rbm/) where we introduced graphical models, Markov random fields (MRFs), and Gibbs/Boltzmann distributions.

In this post we discuss the probability distributions modelled by Boltzmann (and restricted Boltzmann) machines. 

## The distribution of a Boltzmann machine

A *Boltzmann machine* is a machine learning algorithm for learning a certain type of Boltzmann distribution. Before discussing the details of the learning algorithm, let's talk about the distribution that it learns.

**The graph:** The distribution modelled by a Boltzmann machine is a graphical model on a complete graph (every vertex is connected by an edge). The local Markov property of a graphical model on a complete graph is vacuous (cf. our  definition of the local Markov property in [part 1](https://lane203j.github.io/rbm/)). This means there aren't any assumptions about conditional independence built into the model. It also means that there are many parameters to be learned (each edge will correspond to a parameter, and there are many edges). It won't be until we discuss *restricted Boltzmann machines* at the end of this post that the conditional independence assumptions of a MRF appear in an important way.

**The variables:** The random variables \\(x\_i\\) of the graphical model are typically Bernoulli: they take values in \\(\\{0,1\\}\\) with some probability \\(p\_i = p(x\_i=1)\\) (we label the vertices of the graph as \\(1,\ldots, n\\)).

**The joint distribution:** Since it is a Boltzmann distribution, the joint probability distribution we are interested in has the form
\\[
p(x) = \frac{1}{Z}e^{-E(x)}
\\]
and is completely determined by the energy function \\(E\\).  For a Boltzmann machine, we consider energy functions of the form 
\\[E(x) = -\frac{1}{2}x^TWx -b^Tx = -\left(\sum\_{i < j} w\_{i,j}x\_ix\_j + \sum\_j b\_jx\_j\right).\\] 
Here \\(W =  (w\_{i,j})\\) is a symmetric matrix with zeros on the diagonal and \\(b\\) is a vector of length n.  \\(W\\) and \\(b\\) are the parameters of the model that are learned.

Let's summarize the important details.

> The distributions modelled by Boltzmann machines are Boltzmann distributions with the following properties:
> 1. The graph of the graphical model is a complete graph on \\(n\\) vertices.
> 2. The random variables \\(x\_i\\) are Bernoulli.
> 3. The energy function has the form
> \\[E(x) = -\frac{1}{2}x^TWx -b^Tx\\]

In order to understand the meaning of the parameters \\(W,b\\), let's look at how they affect the conditional probability that a random variable \\(x\_i\\) is 1, given all the other random variables (the result of this calculation is also useful in the implementation of Boltzmann machines). First,
\\[
p(x\_i = 1 | x\_j, j\neq i) = \frac{p(x\_i = 1 , x\_j)}{p( x\_j)}.
\\]
The right side expands to give
\\[
\frac{e^{-E(x\_i = 1 , x\_j, j\neq i)}}{e^{-E(x\_i = 1 , x\_j, j\neq i)}+e^{-E(x\_i = 0 , x\_j, j\neq i)}} = \frac{1}{1+e^{E(x\_i = 1 , x\_j, j\neq i)-E(x\_i = 0 , x\_j, j\neq i)}} = \frac{1}{1+e^{-\Delta E_i}}
\\]
where \\(\Delta E\_i\\) is the difference in energies between the two states where \\(x\_i\\) is 1 or 0 and the other random variables are all fixed. Expanding the definition of energy given above, we get
\\[
\Delta E_i = E(x\_i=0,x\_j,j\neq i) - E(x\_i=1,x\_j,j\neq i) = \sum\_{j\neq i}w\_{i,j}x_j + b_i.
\\]
Thus the conditional probability we're interested in is
\\[
p(x\_i = 1 | x\_j, j\neq i) = \sigma\left(\sum\_{j\neq i}w\_{i,j}x_j + b_i\right),
\\]
where \\(\sigma\\) is the *sigmoid function*
\\[
\sigma(x) = \frac{1}{1+e^{-x}}
\\]

This tells us that:

- If \\(w\_{i,j} > 0\\), then when \\(x\_j=1\\) it increases the probability that \\(x\_i=1\\), and the amount that the probability increases scales according to the magnitude of \\(w\_{i,j}\\). If \\(w\_{i,j} < 0\\) it works the other way around.
- Because \\(W\\) is symmetric, this works the other way as well: if \\(w\_{i,j} > 0\\), then \\(p(x\_j=1 | x\_i=1) > p(x\_j=1 | x\_i=0)\\).

- If \\(w\_{i,j} = 0\\), then \\(x\_i\\) is *conditionally independent* of \\(x\_j\\) given all the other random variables in the model (note that \\(w\_{i,j} = 0\\) **does not** imply \\(x\_i\\) and \\(x\_j\\) are independent). This is the same (from the perspective of graphical models) as if we deleted the edge between vertex i and j.

- The \\(b\_i\\) have the effect of biases.


## An aside on visible and hidden variables

In the real world, we study observed data drawn from a probability distribution \\(p(v)\\) (here \\(v\\) stands for "visible," meaning it's the part of the data we see). For instance, if our observed data is a set of pictures of cats, then the probability distribution we're sampling, \\(p(v)\\), is the distribution of all cat photos. 

What a photo of a cat (a sample drawn from \\(p(v)\\)) looks like depends on many other factors: the time of day the photo was taken, the distance between the photographer and the cat, the breed of cat, the pose of the cat (e.g. is it standing or lying down), the place the photo was taken, and so on. 

We can account for these factors by considering  \\(p(v)\\) as a marginal distribution of a joint probability distribution \\(p(v,h)\\) in our model. The extra random variables \\(h\\) are "hidden" because we don't observe them directly when we sample from the observed marginal distribution \\(p(v)\\), but they effect what our samples look like. 

For instance, suppose one of our hidden variables \\(h\_1\\) corresponds to whether the photo was taken at night (\\(h\_1 = 0\\)) or daytime (\\(h\_1 = 1\\)). Samples from the marginal distribution \\(p(v)\\) can be separated into two sets: samples drawn from  the conditional distributions
\\[
p(v|h\_1=0)
\\]
and 
\\[
p(v|h\_1=1)
\\]
and samples from these two distributions will probably look very different. For instance, we would expect samples from the night time distribution to be darker on average than samples from the day time distribution (of course, this also depends on whether the photo is taken indoors or outdoors,  another hidden variable). 

## Visible and hidden vertices in Boltzmann machines

In a Boltzmann machine, the vertices are separated into visible and hidden vertices, and the corresponding random variables are denoted by \\(v\\) and \\(h\\). The meaning of the random variables corresponding to visible vertices is specified as part of the design of the model (e.g. we can associate one visible variable to each black/white pixel in our cat photos). The number of visible vertices is therefore fixed by the dimension of the data samples. The number of hidden vertices, on the other hand, is a design choice.

In the training algorithm, the model is given a set of samples \\(s\_i\\) and learns a joint probability distribution \\(p(v,h|W,b) = \frac{1}{Z}e^{-E(v,h)}\\) that maximizes the likelihood 
\\[
 \prod_i p(v=s\_i|W,b) 
\\]
of the marginal distribution \\(p(v|W,b)\\). We will return to how this is done in the next post.

Part of the magic of Boltzmann machines is that we don't specify the meaning of the hidden variables in the design of the model. The hidden variables' meanings are learned as the model is trained.  This is the same as the hidden layers of any other neural network.  

The interpretation of the hidden vertices of a trained Boltzmann machine can sometimes be understood by looking at the corresponding weights and biases. For example, returning to the cat picture example of the previous section, suppose there is a hidden vertex \\(h\_1\\) with 
\\[
w\_{1,i} = 1 \mbox{ and } b\_1 = -(\text{number of pixels})/2
\\]
If \\(v\_i = 0\\) corresponds to a black pixel and \\(v\_i = 1\\) corresponds to a white pixel, then 
\\[
p(h\_1 = 1 | \text{most pixels are black}) \sim \sigma(  -(\text{number of pixels})/2 )\sim 0
\\]
and
\\[ 
p(h\_1 = 1 | \text{most pixels are white}) \sim \sigma(  (\text{number of pixels})/2 )\sim 1.
\\]
The hidden variable \\(h\_1\\) thus depends on the brightness of the cat image. We *could* interpret \\(h\_1\\) as being similar to our hypothetical day/night hidden variable that we described above.

## Restricted Boltzmann Machines

A *restricted Boltzmann Machine (RBM)* is a Boltzmann machine with extra conditional independence assumptions on the Boltzmann distribution. As we will see, these assumptions correspond to a more sparse graph, and therefore fewer parameters. 

For a RBM we make two assumptions:

1. The visible variables are independent given the hidden variables. i.e.
\\[ p(v\_i ,v\_j  |h) =  p(v\_i |h)p(v\_j  |h).  \\]
2. The hidden variables are independent given the visible variables. i.e.
\\[ p(h\_i ,h\_j  |v) =  p(h\_i |v)p(h\_j  |v).\\]

This is equivalent to requiring that the graph of the corresponding MRF is bipartite, with the vertices separated into two groups: hidden and visible. With these conditions, the conditional probabilities that we calculated at the beginnin of the post simplify to give:
\\[
p(v\_i = 1 | x\_j, j\neq i) = p(v\_i = 1 | h) =  \sigma\left(\sum\_{j}w\_{i,j}h_j + a_i\right)
\\]
\\[
p(h\_j = 1 | x\_i, j\neq i) = p(h\_j = 1 | v) =  \sigma\left(\sum\_{i}w\_{i,j}v_i + b_j\right)
\\]
(here we write a and b for the biases of the visible and hidden vertices, respectively).

## Part 3

In part 3, we will discuss the learning algorithm for RBMs.

